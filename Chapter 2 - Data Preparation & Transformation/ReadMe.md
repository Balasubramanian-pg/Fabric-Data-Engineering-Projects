
[2. Analyze Spark.md](https://github.com/Balasubramanian-pg/Fabric-Data-Engineering-Projects/blob/main/Chapter%202%20-%20Data%20Preparation%20%26%20Transformation/2.%20Analyze%20Spark.md) Run distributed PySpark notebooks for large scale data profiling.

[8.0 Data Science Get Started.md](https://github.com/Balasubramanian-pg/Fabric-Data-Engineering-Projects/blob/main/Chapter%202%20-%20Data%20Preparation%20%26%20Transformation/8.0%20Data%20Science%20Get%20Started.md) Set up a Fabric data science workspace and attach a lakehouse.

[8.1 Data Science Explore Data.md](https://github.com/Balasubramanian-pg/Fabric-Data-Engineering-Projects/blob/main/Chapter%202%20-%20Data%20Preparation%20%26%20Transformation/8.1%20Data%20Science%20Explore%20Data.md) Use notebooks and visualizations to understand dataset characteristics.

[8.2 Data Science Preprocess Data Wrangler.md](https://github.com/Balasubramanian-pg/Fabric-Data-Engineering-Projects/blob/main/Chapter%202%20-%20Data%20Preparation%20%26%20Transformation/8.2%20Data%20Science%20Preprocess%20Data%20Wrangler.md) Clean and transform features with the point and click Wrangler tool.

[8.3 Data Science Train.md](https://github.com/Balasubramanian-pg/Fabric-Data-Engineering-Projects/blob/main/Chapter%202%20-%20Data%20Preparation%20%26%20Transformation/8.3%20Data%20Science%20Train.md) Build and tune machine learning models in a Spark notebook.

[8.4 Data Science Batch.md](https://github.com/Balasubramanian-pg/Fabric-Data-Engineering-Projects/blob/main/Chapter%202%20-%20Data%20Preparation%20%26%20Transformation/8.4%20Data%20Science%20Batch.md) Schedule batch scoring pipelines to generate predictions on new data.

This series of components describes the next stage in your technical development. It is where raw computational power meets disciplined inquiry. Each step is an attempt to transform potential into structured understanding.

The Spark analysis layer represents the confrontation with scale. You don’t escape complexity by ignoring it. You face it with distributed computation and methodical profiling, letting the data reveal its character.

The data science workspace is the environment you prepare before any meaningful transformation can occur. Like any workshop, it must be properly set up or everything built within it will be unstable.

Exploration comes next. Notebooks, visualizations, careful observation. You look at the data the way you might examine a problem in your own life. You map its boundaries. You identify its anomalies. You learn what you’re truly dealing with.

The Wrangler step introduces refinement. It is the disciplined act of cleaning, shaping, and clarifying. Chaos is pared down until only the useful remains.

Model training is the attempt to extract a principle from the particulars. You build something that can generalize, something that can understand new information without collapsing. That is not a trivial task. It mirrors the process of building a worldview capable of handling reality.

Finally, batch scoring brings the system full circle. Your models are not decorative. They must act in the world repeatedly, consistently, and predictably. That means scheduling, automation, and responsibility for the outcomes.

Together, these steps form a developmental arc. You begin in complexity, impose structure, derive insight, and return that insight back into practice. That is how real progress is made.