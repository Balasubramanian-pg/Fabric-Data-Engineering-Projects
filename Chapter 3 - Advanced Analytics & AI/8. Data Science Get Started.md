# Data Science in Microsoft Fabric - Lab Guide

## Overview

This lab guides you through the data science capabilities in Microsoft Fabric, including data ingestion, exploration, processing, and machine learning model training. You'll gain hands-on experience with:

- Working with notebooks in Microsoft Fabric
- Using Data Wrangler for data preparation
- Training both regression and classification models
- Tracking experiments with MLflow
- Managing and comparing machine learning models

**Estimated Completion Time**: 20 minutes

> [!NOTE]
> You need a [Microsoft Fabric trial](https://learn.microsoft.com/fabric/get-started/fabric-trial) to complete this exercise.

## Prerequisites

- Microsoft Fabric trial account
- Basic understanding of Python and machine learning concepts

## Lab Exercises

### 1. Create a Workspace

1. Navigate to the [Microsoft Fabric home page](https://app.fabric.microsoft.com/home?experience=fabric)
2. Select **Workspaces** (&#128455;) from the left menu
3. Create a new workspace with:
   - Name of your choice
   - Licensing mode that includes Fabric capacity (Trial, Premium, or Fabric)

![Empty workspace in Fabric](./Images/new-workspace.png)

### 2. Create and Configure a Notebook

1. Select **Create** > **Notebook** under Data Science section
2. Rename the notebook to your preference
3. Convert the first cell to markdown and add:
   ```markdown
   # Data science in Microsoft Fabric
   ```

### 3. Ingest and Explore Data

1. Add and run code to load the diabetes dataset:
   ```python
   # Azure storage access info
   blob_account_name = "azureopendatastorage"
   blob_container_name = "mlsamples"
   blob_relative_path = "diabetes"
   blob_sas_token = r""
   
   # Configure Spark access
   wasbs_path = f"wasbs://%s@%s.blob.core.windows.net/%s" % (blob_container_name, blob_account_name, blob_relative_path)
   spark.conf.set("fs.azure.sas.%s.%s.blob.core.windows.net" % (blob_container_name, blob_account_name), blob_sas_token)
   
   # Read data
   df = spark.read.parquet(wasbs_path)
   ```

2. Display and visualize the data:
   ```python
   display(df)
   ```

> [!TIP]
> The first Spark code execution may take a minute as the Spark pool initializes.

### 4. Prepare Data with Data Wrangler

1. Convert to Pandas DataFrame:
   ```python
   df = df.toPandas()
   ```

2. Use Data Wrangler to:
   - Review data summary
   - Create a new binary `Risk` column based on the 75th percentile of Y:
     ```python
     (df['Y'] > 211.5).astype(int)
     ```

### 5. Train Machine Learning Models

#### Regression Model
```python
from sklearn.linear_model import LinearRegression
import mlflow

mlflow.set_experiment("diabetes-regression")

with mlflow.start_run():
    mlflow.autolog()
    model = LinearRegression()
    model.fit(X_train, y_train)
```

#### Classification Model
```python
from sklearn.linear_model import LogisticRegression

mlflow.set_experiment("diabetes-classification")

with mlflow.start_run():
    mlflow.sklearn.autolog()
    model = LogisticRegression(C=1/0.1, solver="liblinear").fit(X_train, y_train)
```

### 6. Explore and Save Models

1. Navigate to your workspace to view experiments
2. Compare metrics between regression and classification models
3. Save the best performing model:
   - Select "Save as ML model"
   - Name it `model-diabetes`

> [!IMPORTANT]
> Model artifacts, parameters, and metrics are automatically tracked by MLflow.

## Cleanup

1. Rename your notebook to "Train and compare models"
2. Stop the Spark session
3. Optionally delete the workspace if no longer needed:
   - Workspace Settings > Remove this workspace

## Next Steps

- Explore deploying your model for real-time predictions
- Try different algorithms and hyperparameters
- Investigate feature importance for your models

> [!WARNING]
> Remember that workspace deletion is permanent and will remove all contained items.
